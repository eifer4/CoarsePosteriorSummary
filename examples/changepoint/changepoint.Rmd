---
title: "Adapting the posterior from a naive linear model to a change point model"
output: html_document
author: Eric Dunipace
date: May 5, 2019
---
<style>
p.caption {
  font-size: 0.9em;
  font-style: italic;
  color: grey;
  margin-right: 10%;
  margin-left: 10%;  
  text-align: justify;
}
</style>
In some situations, we may which to adapt a naively calculated posterior from a linear regression to a reference model that fits better. This could be the output of a more complicated model, such as a Bayesian Model Averaging (BMA), a neural network, or a regression tree model. In some cases, these model may not be very interpretable, so we may wish to obtain a linear model that performs well for a subset of the data but which is adapted to the better predicting model. This may give us a sense of variable importance or just give us better predictions from a more interpretable model.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r change point data, echo = FALSE, message=FALSE, warning=FALSE}
require(ggplot2)
require(SparsePosterior)
require(CoarsePosteriorSummary)

#### load functions ####
set.seed(57)
n <- 1024
p <- 2
nsamp <- 1000
nlambda <- p*10
lambda.min.ratio <- 1e-10
penalty.factor <- rep(1,2)
gamma <- 1
pseudo.observations <- 0

target <- get_normal_linear_model()
hyperparameters <- list(mu = NULL, sigma = NULL,
                          alpha = NULL, beta = NULL,
                          Lambda = NULL)
  hyperparameters$mu <- rep(0,p)
  hyperparameters$sigma <- diag(1,p,p)
  hyperparameters$alpha <- 10
  hyperparameters$beta <- 10
  hyperparameters$Lambda <- solve(hyperparameters$sigma)
  
target$X$corr <- 0
X <- target$X$rX(n, target$X$corr, p)
param <- target$rparam()
param$theta <- c(2,0.1)
param$sigma2 <- 25

X2 <- X[,2,drop=FALSE]
less0 <- X2<0
Y <- ifelse(!less0, target$rdata(n,X2, param$theta[1], param$sigma2)$Y,
            target$rdata(n,X2, param$theta[2], param$sigma2))

fullX <- cbind(as.numeric(X[,2]>0),
               as.numeric(X[,2]>0) * X[,2],
               as.numeric(X[,2]<=0),
               as.numeric(X[,2]<=0) * X[,2])


true_mu <- ifelse(!less0, X2 %*% c(param$theta[1]), X2 %*% param$theta[2])


```

### Simulations
We simulate $n = `r n`$ observations from a change point model. There is one predictor variable, $X \sim N(0,1)$ and if $X <0$ then $Y \sim N(X \cdot \beta_1, \sigma^2)$, where $\beta_1 = `r param$theta[1]`$ and $\sigma^2 = `r param$sigma2`$, otherwise $Y \sim N(X \cdot \beta_2, \sigma^2)$ with $\beta_2 = `r param$theta[2]`$ and $\sigma^2 = `r param$sigma2`$. 

```{r data print, eval=FALSE}
<<change point data>>
```


### Posterior Estimation
We estimate our posterior using a conjugate normal model with
\[
\beta \sim N(0, \sigma^2 \mathbf{I})
\] 
and
\[
\sigma^2 \sim \text{Inv-Gamma}(10,10).
\]
We then sample from the posterior `r nsamp` times.

```{r posterior}
fullHyper <- list(mu = NULL, sigma = NULL,
                          alpha = NULL, beta = NULL,
                          Lambda = NULL)
  fullHyper$mu <- rep(0, ncol(fullX))
  fullHyper$sigma <- diag(1, ncol(fullX), ncol(fullX))
  fullHyper$alpha <- 10
  fullHyper$beta <- 10
  fullHyper$Lambda <- solve(fullHyper$sigma)

post <- target$rpost(nsamp, fullX, Y, fullHyper, method = "conjugate")
post.naive <- target$rpost(nsamp, X, Y, hyperparameters, method = "conjugate")

# post.stan <- target$rpost(nsamp, X, Y, hyperparameters, method = "stan",stan_dir ="../Stan/normal_horseshoe_noQR.stan", m0 = 20, scal_intercept = 2.5,
#                           chains = 4)

cond_mu_full <- fullX %*% post$theta
cond_mu_naive <- X %*% post.naive$theta
n0 <- sum(less0)

# xtx <- crossprod(X[less0,])/n0
# xty <- crossprod(X[less0,], cond_mu_full[less0,])/n0
mean_theta <- matrix(rowMeans(post.naive$theta),p,nsamp)
centered_theta <- post.naive$theta-mean_theta
comb_theta <- rbind(centered_theta, mean_theta)
```

### Posterior Adaptation/Variable Importance
We then adapt the posterior using 3 methods: 1) selecting aspects of the posterior that best match the reference model, 2) a location scale adaptiation, and 3) projection of the posterior into the column space of the covariates.
```{r adaptive, cache=TRUE}
theta_selection <- W2L1(X=X[less0,], Y=cond_mu_full[less0,], 
                        theta=post.naive$theta, penalty="selection.lasso",
                        nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                        infimum.maxit=1e4, maxit = 1e3, gamma = gamma,
                        pseudo_observations = pseudo.observations, display.progress = TRUE,
                 penalty.factor = penalty.factor, method="selection.variable")
theta_adaptive <- W2L1(X=X[less0,], Y=cond_mu_full[less0,], 
                        theta=post.naive$theta, penalty="mcp",
                        nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                        infimum.maxit=1e4, maxit = 1e4, gamma = gamma,
                        pseudo_observations = pseudo.observations, display.progress = TRUE,
                 penalty.factor = penalty.factor,
                  method="location.scale")
theta_proj <- W2L1(X=X[less0,], Y=cond_mu_full[less0,], 
                        theta=post.naive$theta, penalty="mcp",
                        nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                        infimum.maxit=50, maxit = 1e4, gamma = gamma,
                        pseudo_observations = pseudo.observations, display.progress = TRUE,
                 penalty.factor = penalty.factor, method="projection")

```
```{r unused, echo=FALSE }
# theta_scale <- W2L1(X=X[less0,], Y=cond_mu_full[less0,], 
#                         theta=post.naive$theta, penalty="mcp",
#                         nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
#                         infimum.maxit=1e4, maxit = 1e3, gamma = gamma,
#                         pseudo_observations = delta, display.progress = TRUE,
#                  penalty.factor = c(1,1), method="scale")
# test <- W2L1(X=X[less0,,drop=FALSE], Y=cond_mu_full[less0,], 
 #                        theta=post.naive$theta, penalty="ols",
 #                        nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
 #                        infimum.maxit=50, maxit = 5e2, gamma = gamma,
 #                        pseudo_observations = 0, display.progress = FALSE,
 #                 penalty.factor = c(1,1), method="projection", lambda=0)
#calc.gamma(xtx = NULL, xty = NULL, active.idx = NULL, 
                       # method = c("projection","identity",
                       # "adaptive", "approx.projection", "KL"), 
                       # x = NULL,
                       # theta = NULL, theta_norm = NULL, pseudo.obs = NULL, Y = NULL, niter=5e2)
# theta_perp <- W2L1(X=X[less0,,drop=FALSE], Y=cond_mu_full[less0,], 
#                         theta=post.naive$theta, penalty="ols",
#                         nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
#                         infimum.maxit=50, maxit = 5e2, gamma = gamma,
#                         pseudo_observations = 0, display.progress = FALSE,
#                  penalty.factor = penalty.factor, method="projection", lambda=0)

```

```{r raw data, echo=FALSE}
# ggplot() + geom_point(aes(y = Y, x = X[,2]))
# ggplot() + geom_point(aes(y = c(cond_mu_full[,sample(1:1000,5)]), x = rep(X[,2],5)), alpha=0.01)
# ggplot() + geom_point(aes(y = c(cond_mu_full[,sample(1:1000,5)]), x = rep(X[,2],5)), alpha=0.8, shape=".") +
# geom_point(aes(y = c(cond_mu_naive[,sample(1:1000,5)]), x = rep(X[,2],5)), alpha=0.8, color="red", shape=".") + xlab("X") +
#   ylab("Predicted Y")

```


```{r figure_gen, echo = FALSE}
nplotsamps <- 10
desired_seq <- sample(1:1000,nplotsamps, replace=FALSE)
sub_samp <- sample(1:sum(less0),50)
nonzero_samp <- sample(1:sum(!less0),50)
plotidx <- which(less0)[sub_samp]
full.plot.idx <- c(plotidx, which(!less0)[nonzero_samp])
repX <- rep(X2[full.plot.idx], nplotsamps)
linegroup <- matrix(1:length(cond_mu_full),nrow=n, ncol=nsamp)
jitter.width <- sd(X2)/100
jitter.height <- sqrt(param$sigma2)/100
pd <- position_dodge(jitter.width)
alpha.point <- 0.1

projected_mu <- X[plotidx,] %*% matrix(theta_proj$beta[,1],p,nsamp)
selected <- X[plotidx,] %*% diag(theta_selection$beta[,1]) %*% post.naive$theta

adapt_theta <- diag(theta_adaptive$beta[1:p,1]) %*% centered_theta + 
                diag(theta_adaptive$beta[(p+1):(2*p),1]) %*% mean_theta
adaptive <- X[plotidx,] %*% adapt_theta
df <- data.frame(Y=Y[plotidx], X = X2[plotidx], proj = c(projected_mu[,desired_seq]),
                 sel = c(selected[,desired_seq]), ada = c(adaptive[,desired_seq]), mu = c(cond_mu_full[plotidx,desired_seq]),
                 naive = c(cond_mu_naive[plotidx,desired_seq]),
                 group = c(linegroup[plotidx,desired_seq]))
plot <- ggplot() + 
  geom_line(aes(y = rowMeans(cond_mu_full), x = X2), color="grey") + 
  geom_ribbon(aes(x = X2, ymin = apply(cond_mu_full,1,quantile, 0.025), ymax = apply(cond_mu_full,1,quantile, 0.975)), color="grey92",alpha =0.2) +
  xlab("X") +
  ylab("Posterior Predictive Mean") + theme_bw()

plot <- plot + 
  geom_jitter(aes(y = c(cond_mu_naive[full.plot.idx,desired_seq]), x = repX), 
              alpha=alpha.point, color="red", shape = 3, 
              width=jitter.width, height=jitter.height,
              size=2)
plot1 <- plot + 
  geom_jitter(aes(y = df$sel, x = df$X),
              alpha=alpha.point, color="blue", 
              shape=20, width=jitter.width, height=jitter.height, size = 2) +
  ggtitle("Selection adapted posterior")

plot2 <-   plot + geom_jitter(aes(x=df$X, y = df$ada), 
                      alpha=alpha.point, color="magenta4", shape = 20,
             width=jitter.width, height=jitter.height, size = 2) +
  ggtitle("Location/Scale adapted posterior")
  # geom_line(aes(x=rep(df$X,2), y = c(df$ada,df$naive), group=rep(df$group,2)),
  #           color="gray43", position=pd,alpha = 0.2)
  
plot3 <- plot +
  geom_jitter(aes(y = df$proj, x = df$X),alpha=alpha.point, color="darkgreen", 
              shape=20, width=jitter.width, height=jitter.height, size = 2) +
  ggtitle("Projected posterior")


```

First, we look at the effect of simply turning on and off aspects of the posterior to get a sense of which coefficients/covariates are important. As we expect, this model doesn't adapt at all to the reference posterior and just returns the original model since it is the best linear fit .
```{r figure_select, echo=FALSE, fig.height= 4, fig.width=4, fig.align = "center",out.width="60%", optipng = '-o7', fig.cap="Figure 1: Adapted posterior selecting the most important covariates for inclusion without any adaptation to the reference posterior (the true model) for X-values less than 0. The gray line and shaded area represent the posterior mean and credible intervals of the posterior from the correct model. The red-hashes are the samples from the posterior predictive mean of the given X-value from the simple linear model while the blue circles are the samples using only the most important covariates. In this case, the two covariates, the intercept and single X-variable, are both equally important and are selected together. For clarity, only 10 posterior samples have been plotted for 50 X-values."}
print(plot1)
```

Then we try change the mean and variance of the naive-linear posterior as one would in a variational Bayesian approach. This works well since the posterior is a t-distribution and we can see the adapated samples fall in the region of the true posterior.
```{r figure_locscale, echo=FALSE,fig.height= 4, fig.width=4, fig.align = "center", out.width="60%", optipng = '-o7', fig.cap="Figure 2: Adapted posterior using a location/scale transform to change the first and second moments of the posterior distribution of the regression coefficients to closer match that of the reference posterior (the correct model) for X-values less than 0. Similar to a variational Bayes approach, this has the effect of moving the center of the posterior coefficients and scaling them to approximately match the the reference posterior. The red-hashes are the samples from the posterior predictive mean of the given X-value from the simple linear model while the brown circles are the samples using the location/scale adapation of the posterior predictive mean. The intercept was dropped from the model. For clarity, only 10 posterior samples have been plotted for 50 X-values."}
print(plot2)
```

```{r figure_project, echo=FALSE, fig.height= 4, fig.width=4, out.width="60%", optipng = '-o7', fig.align = "center", fig.cap="Figure 3: Adapted posterior using a projection of the reference posterior (the correct model) into the space of the intercept and X-values less than 0. The intercept was dropped from the model. The red-hashes are the samples from the posterior predictive mean of the given X-value from the simple linear model while the green circles are the samples using the projection adapation of the posterior predictive mean. For clarity, only 10 posterior samples have been plotted for 50 X-values."}
print(plot3)
```


