---
title: "Gaussian Data Example"
author: "Eric Dunipace"
date: "6/18/2019"
output:
  pdf_document: default
  # html_document: default
header-includes: \usepackage{float}
---
<style>
p.caption {
  font-size: 0.9em;
  font-style: italic;
  color: grey;
  margin-right: 10%;
  margin-left: 10%;  
  text-align: justify;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
require(SparsePosterior)
require(ggplot2)
require(CoarsePosteriorSummary)
require(rstan)
require(ggsci)
require(doParallel)
require(ggridges)
require(data.table)
```
```{r data, message= FALSE, warning = FALSE, echo = FALSE}
#### Load packages ####
require(SparsePosterior)
require(ggplot2)
require(CoarsePosteriorSummary)
require(rstan)
require(ggsci)
require(doParallel)
require(ggridges)
require(data.table)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores()-1)

group.names <- c("Selection","Loc./Scale", "Projection")

#### generate data ####
set.seed(9507123)
n <- 1024
p <- 31
nsamp <- 1000
nlambda <- 100
lambda.min.ratio <- 1e-10
gamma <- 1
pseudo.observations <- 0

target <- get_normal_linear_model()
param <- target$rparam()
p_star <- length(param$theta)

target$X$corr <- 0.5
X <- target$X$rX(n, target$X$corr, p)
data <- target$rdata(n,X[,1:p_star], param$theta, param$sigma2)
Y <- data$Y

x <- target$X$rX(1, target$X$corr, p)

true_mu_full <- data$mu
true_mu <- x[,1:p_star,drop=FALSE] %*% c(param$theta)
```
```{r posterior, message = FALSE, warning = FALSE, echo = FALSE}

hyperparameters <- list(mu = NULL, sigma = NULL,
                          alpha = NULL, beta = NULL,
                        Lambda = NULL)
hyperparameters$mu <- rep(0,p)
hyperparameters$sigma <- diag(1,p,p)
hyperparameters$alpha <- 10
hyperparameters$beta <- 10
hyperparameters$Lambda <- solve(hyperparameters$sigma)
  
# posterior <- target$rpost(nsamp, X, Y, method = "stan",
#                           stan_dir ="../../Stan/normal_horseshoe_noQR.stan",
#                           m0 = 20, scale_intercept = 2.5, chains = 4) # this can take a bit

posterior <- target$rpost(nsamp, X, Y, X.test = x, hyperparameters = hyperparameters, method="conjugate")


cond_mu <- posterior$test$mu
cond_mu_full <- posterior$mu
```
```{r method_in_sample, message=FALSE, warning=FALSE, echo=FALSE}
penalty.factor <- rep(1,p)
force <- NULL
adapt_file <- "gaussian_adapt.RData"
if(!file.exists(adapt_file)){
  theta_selection <- W2L1(X=X, Y = cond_mu_full, 
                          theta=posterior$theta, penalty="selection.lasso",
                          nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                          infimum.maxit=1e4, maxit = 1e3, gamma = gamma,
                          pseudo_observations = 0, display.progress = TRUE,
                   penalty.factor = penalty.factor, method="selection.variable")
  
  theta_proj <- W2L1(X=X, Y=cond_mu_full, 
                          theta=posterior$theta, penalty="mcp",
                          nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                          infimum.maxit=1, maxit = 1e3, gamma = gamma,
                          pseudo_observations = pseudo.observations, display.progress = TRUE,
                   penalty.factor = penalty.factor, method="projection")
  
  theta_SA <- WPSA(X=X, Y=cond_mu_full, 
                        theta=posterior$theta, force = force,
                 p = 2, model.size = c(1:p),
                 iter=10, temps = 10,
                 pseudo.obs = 0,
                 proposal = proposal.fun,
                 options = list(method = c("selection.variable"),
                                energy.distribution = "boltzman",
                                cooling.schedule = "exponential"),
                 display.progress = TRUE
                  )
  theta_SW <- WPSW(X=X, Y=cond_mu_full, 
                          theta=posterior$theta, force = force, p = 2,
                          direction = "backward", 
                          method="selection.variable",
                   display.progress = TRUE
                  )
  save(theta_proj, theta_selection, theta_SA, theta_SW, file=adapt_file)
} else {
  load(adapt_file)
}

```
```{r w2 distances, message=FALSE, warning=FALSE, echo=FALSE}
models <- list(Selection = theta_selection, 
               Projection = theta_proj,
               "Simulated Annealing" = theta_SA,
               "Backward Stepwise" = theta_SW)
gauss_plot_file <- "w2plot.rds"
if ( !file.exists(gauss_plot_file) ) { #only run if file doesn't exist
  w2plot <- distCompare(models, list(posterior = posterior$theta, 
                                     mean = cond_mu_full),
                       method = c("exact", "exact"),
                       quantity = c("posterior","mean"), 
                       parallel = TRUE) # this can take a LONG time
  saveRDS( w2plot, gauss_plot_file )
} else {
  w2plot <- readRDS( gauss_plot_file )
}
  w2plots <- plot(w2plot, ylab = "2-Wasserstein Distance")
```
```{r mse distances, message=FALSE, warning=FALSE, echo=FALSE}
models <- list(Selection = theta_selection, 
               Projection = theta_proj,
               "Simulated Annealing" = theta_SA,
               "Backward Stepwise" = theta_SW)
msefile <- "mseplot.rds"
if (!file.exists(msefile)) {
  mse_plot <- distCompare(models, list(posterior = c(param$theta, 
                                                     rep(0,p-p_star)), 
                                     mean = true_mu_full),
                       method = c("mse", "mse"),
                       quantity = c("posterior","mean"), 
                       parallel = TRUE) # this can take a LONG time
  saveRDS( mse_plot, msefile )
} else {
  mse_plot <- readRDS(msefile)
}

mse_plots <- plot(mse_plot, ylab = "MSE")
```
```{r ridgeplot, echo = FALSE, warning = FALSE, messages = FALSE}
models <- list(Selection = theta_selection, 
               Projection = theta_proj,
               "Simulated Annealing" = theta_SA,
               "Backward Stepwise" = theta_SW)
idx <- 512
ridge512 <- ridgePlot(models, index=idx, minCoef = 7, maxCoef = 10, scale =1 , alpha =0.5, full = cond_mu_full)

ranks <- ranking(models$Projection, cond_mu_full, p = 2, minCoef = 7, maxCoef = 10, quantiles = c(0,0.3, 0.67, 1))
ridgePlots <- ridgePlot(models, index=ranks$index, minCoef = 7, maxCoef = ranks$ncoef, scale =1 , alpha =0.5, full = cond_mu_full)

ranks2 <- ranking(models$Projection, cond_mu_full, p = 2, maxCoef = 6, quantiles = c(0,0.3, 0.67, 1))
ridgePlots2 <- ridgePlot(models, index=ranks2$index, maxCoef = ranks2$ncoef, scale =1 , alpha =0.5, full = cond_mu_full)
```

## Setup
### Predictors
We can measure variable importance in a variety of settings. Our first example is in a setting with Normally distributed data. We generate $n = `r n`$ predictor variables, $X$, from a Multivariate Normal,
\[ X_i \sim N(0, \Sigma), \] with $\Sigma_{i,i} = 1$ for all $i$ from 1 to $p = `r p-1`.$ The first 5 dimensions of $X$ are correlated, dimensions 6-10 are correlated, dimensions 10-20 are correlated, and dimensions 21 to 30 are correlated with correlations `r target$X$corr`. Thus, $\Sigma$ is block diagonal.

### Parameters
We generate parameters as follows
\[ \beta_{0:5} \sim \text{Unif}(1,2),\]
\[ \beta_{6:10} \sim \text{Unif}(-2,-1), \] 
\[ \beta_{11:20} \sim \text{Unif}(0,0.5), \] 
\[ \beta_{21:30} = 0 ,\] and
\[\sigma^2 = 1.\]

### Outcome
We sample the outcome as
\[Y_i \sim N(\beta_0 + X_i^{\top} \beta_{1:20}, \sigma^2) \]

```{r data_echo, echo=TRUE, eval=FALSE}
<<data>>
```

## Posterior Estimation
We put conjugate priors on our parameters,
\[
  \beta_{0:p} \sim N(0,1)
\] and
\[
  \sigma^2 \sim \text{Inv-Gamma}(10,10),
\]
which facillitates quick estimation via known posterior distributions.
```{r post print, eval=FALSE}
hyperparameters <- list(mu = NULL, sigma = NULL,
                          alpha = NULL, beta = NULL,
                        Lambda = NULL)
hyperparameters$mu <- rep(0,p)
hyperparameters$sigma <- diag(1,p,p)
hyperparameters$alpha <- 10
hyperparameters$beta <- 10
hyperparameters$Lambda <- solve(hyperparameters$sigma)

posterior <- target$rpost(nsamp, X, Y, hyperparameters = hyperparameters, method="conjugate")


cond_mu <- x %*% posterior$theta
cond_mu_full <- X %*% posterior$theta
```


We then utilize our 1) selection variable, 2) location/scale, and 3) projection methods to find coarse posteriors close to our full one. Note: this code may take a bit to run.

```{r adapt print, eval = FALSE}
penalty.factor <- rep(1, p)
force <- NULL

theta_selection <- W2L1(X=X, Y = cond_mu_full, 
                    theta=posterior$theta, penalty="selection.lasso",
                    nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                    infimum.maxit=1e4, maxit = 1e3, gamma = gamma,
                    pseudo_observations = 0, display.progress = TRUE,
                    penalty.factor = penalty.factor, 
                    method="selection.variable")

theta_proj <- W2L1(X=X, Y=cond_mu_full, 
                    theta=posterior$theta, penalty="mcp",
                    nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                    infimum.maxit=1, maxit = 1e3, gamma = gamma,
                    pseudo_observations = pseudo.observations, 
                    display.progress = TRUE,
                    penalty.factor = penalty.factor, method="projection")

theta_SA <- WPSA(X=X, Y=cond_mu_full, 
                    theta=posterior$theta, force = force,
                    p = 2, model.size = c(1:p),
                    iter=10, temps = 10,
                    pseudo.obs = 0,
                    proposal = proposal.fun,
                    options = list(method = c("selection.variable"),
                                  energy.distribution = "boltzman",
                                  cooling.schedule = "exponential"),
                    display.progress = TRUE)

theta_SW <- WPSW(X=X, Y=cond_mu_full, 
                    theta=posterior$theta, force = force, p = 2,
                    direction = "backward", 
                    method="selection.variable",
                    display.progress = TRUE)
```

## Distance Plots
It is useful to look at plots to see how our coarsened posteriors are doing. We can look at how close the coarsened posterior parameters are to the full posterior parameters in terms of 2-Wasserstein distance 
```{r dist_plot_post,echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=4, fig.pos = "H", out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold',fig.pos="H", fig.cap="2-Wasserstein distance between the full posterior for the regression coefficients and our coarsened version."}
print(w2plots$posterior)
```

and how far away the coarsened posteriors are in terms of mean-squared error (MSE) from the true data generating parameters.

```{r mse_plot_post,echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=4, fig.pos = "H", out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold', fig.pos="H",fig.cap="MSE between the full posterior for the regression coefficients and the true data generating parameters."}
print(mse_plots$posterior)
```


Moreover, we can look at the differences the posterior predictive mean and the full posterior predictive mean, as well as the MSE between the coarse posterior and true mean.
```{r dist_plot_mean,echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=4, fig.pos = "H", out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold', fig.pos="H", fig.cap="2-Wasserstein distance between coarse and full posterior predictive means and MSE between posterior predictive means and the true means (calculated from the true parameters)"}
print(w2plots$mean)
print(mse_plots$mean)
```


We can see as more covariates are active, the coarse distribution gets close to the full posterior and the true mean. 

## Ridgeline Plots
We may be interested to see how well the coarasened posterior is approximating an individuals mean, relative to the full model, for a given model size. To examine this relationship, we can look at ridgeline plots of the posterior predictive means from the coarsened posteriors. Take, for example, individual $i = 512$.
```{r ridgeplot_echo, eval=FALSE}
idx <- 512
ridge512 <- ridgePlot(models, index=idx, maxCoef = 10, 
                      scale =1 , alpha =0.5, full = cond_mu_full)
```
```{r ridge_plots_single, echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=8, out.width="90%", optipng = '-o7', fig.align = "center"}
print(ridge512)
```
We can see that as we add more coefficients up to our desired size, 10, the models perform better. For model size 10, there is no prediction for the Projection method, but it looks like the backward stepwise method performs best. If we really wanted to include the Projection method, we could try model size 9 or try a larger model size where it was present.

Of course, we may be interested in looking at how good or how bad the predictions from the coarsened posterior can be for the entire sample, and not just one person chosen at random. Towards this end, we have implemented a function that for a given model size, will calculate the average p-Wasserstein distance between the coarsened predictions and the full posterior, and rank all individual observations by this distance. Then this ranking function will give the observations at user provided quantiles.

```{r rank fun, eval=FALSE}
ranks <- ranking(models$Projection, cond_mu_full, p = 2, 
                 maxCoef = 10, quantiles = c(0,0.3, 0.67, 1))
```



Next, we can plot ridgeline plots of the predictions from 1 coefficient up to the given model size. This allows us to see if smaller models perform equally well.
```{r ridge print, eval = FALSE}
ridgePlots <- ridgePlot(models, index=ranks$index, 
                        maxCoef = ranks$ncoef, scale =1 , alpha =0.5, full = cond_mu_full)
print(ridgePlots)
```


```{r rigeplot_print, echo=FALSE, message=FALSE, warning=FALSE, fig.height= 8, fig.width=8, out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold', fig.pos = "H", fig.cap = "Ridgeline plots for 1 to 10 coefficients. The top left plot is the observation whose coarsened posterior prediction is closest to the true posterior predictive distribution in terms of 2-Wasserstein distance ($0^{th}$ percentile in terms of ranks), and the bottom right is the observations whose is furthest from the true posterior predictive distribution ($100^{th}$ percentile in terms of ranks). Top row, left to right: $0^{th}$ and $33^{rd}$ percentiles. Bottom row, left to right: $67^{th}$ and $100^{th}$ percentiles."}
ridgePlots[[1]]
ridgePlots[[2]]
ridgePlots[[3]]
ridgePlots[[4]]
```

If we had wanted a smaller model size, say 6, that would be possible as well

```{r ridgeplot 2, eval = FALSE}
ranks2 <- ranking(models, cond_mu_full, p = 2, 
                  maxCoef = 6, quantiles = c(0,0.3, 0.67, 1))
ridgePlots2 <- ridgePlot(models, index=ranks2$index, 
                         maxCoef = ranks2$ncoef, scale =1 , alpha =0.5, full = cond_mu_full)
print(ridgePlots2)
```



```{r rigeplot_print2, echo=FALSE, message=FALSE, warning=FALSE, fig.height= 8, fig.width=8, out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold', fig.pos = "H", fig.cap = "Ridgeline plots for 1 to 6 coefficients. The top left plot is the observation whose coarsened posterior prediction is closest to the true posterior predictive distribution in terms of 2-Wasserstein distance ($0^{th}$ percentile in terms of ranks), and the bottom right is the observations whose is furthest from the true posterior predictive distribution ($100^{th}$ percentile in terms of ranks). Top row, left to right: $0^{th}$ and $33^{rd}$ percentiles. Bottom row, left to right: $67^{th}$ and $100^{th}$ percentiles."}
ridgePlots2[[1]]
ridgePlots2[[2]]
ridgePlots2[[3]]
ridgePlots2[[4]]
