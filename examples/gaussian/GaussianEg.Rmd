---
title: "Gaussian Data Example"
author: "Eric Dunipace"
date: "5/6/2019"
output:
  pdf_document: default
  html_document: default
---
<style>
p.caption {
  font-size: 0.9em;
  font-style: italic;
  color: grey;
  margin-right: 10%;
  margin-left: 10%;  
  text-align: justify;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(SparsePosterior)
require(ggplot2)
require(CoarsePosteriorSummary)
require(rstan)
require(ggsci)
require(doParallel)
require(ggridges)
require(data.table)
```
```{r data, message= FALSE, warning = FALSE, echo = FALSE}
#### Load packages ####
require(SparsePosterior)
require(ggplot2)
require(CoarsePosteriorSummary)
require(rstan)
require(ggsci)
require(doParallel)
require(ggridges)
require(data.table)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores()-1)

group.names <- c("Selection","Loc./Scale", "Projection")

#### generate data ####
set.seed(9507123)
n <- 1024
p <- 31
nsamp <- 1000
nlambda <- 100
lambda.min.ratio <- 1e-10
gamma <- 1
pseudo.observations <- 0

target <- get_normal_linear_model()
param <- target$rparam()
p_star <- length(param$theta)

target$X$corr <- 0.5
X <- target$X$rX(n, target$X$corr, p)
Y <- target$rdata(n,X[,1:p_star], param$theta, param$sigma2)

x <- target$X$rX(1, target$X$corr, p)

true_mu_full <- X[,1:p_star, drop=FALSE] %*% c(param$theta)
true_mu <- x[,1:p_star,drop=FALSE] %*% c(param$theta)
```
```{r posterior, message = FALSE, warning = FALSE, echo = FALSE}

hyperparameters <- list(mu = NULL, sigma = NULL,
                          alpha = NULL, beta = NULL,
                        Lambda = NULL)
hyperparameters$mu <- rep(0,p)
hyperparameters$sigma <- diag(1,p,p)
hyperparameters$alpha <- 10
hyperparameters$beta <- 10
hyperparameters$Lambda <- solve(hyperparameters$sigma)
  
# posterior <- target$rpost(nsamp, X, Y, method = "stan",
#                           stan_dir ="../../Stan/normal_horseshoe_noQR.stan",
#                           m0 = 20, scale_intercept = 2.5, chains = 4) # this can take a bit

posterior <- target$rpost(nsamp, X, Y, hyperparameters = hyperparameters, method="conjugate")


cond_mu <- x %*% posterior$theta
cond_mu_full <- X %*% posterior$theta
```
```{r method_in_sample, message=FALSE, warning=FALSE, echo=FALSE}
penalty.factor <- rep(1,p)
force <- NULL
adapt_file <- "gaussian_adapt.RData"
if(!file.exists(adapt_file)){
  theta_selection <- W2L1(X=X, Y = cond_mu_full, 
                          theta=posterior$theta, penalty="selection.lasso",
                          nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                          infimum.maxit=1e4, maxit = 1e3, gamma = gamma,
                          pseudo_observations = 0, display.progress = TRUE,
                   penalty.factor = penalty.factor, method="selection.variable")
  
  theta_proj <- W2L1(X=X, Y=cond_mu_full, 
                          theta=posterior$theta, penalty="mcp",
                          nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                          infimum.maxit=1, maxit = 1e3, gamma = gamma,
                          pseudo_observations = pseudo.observations, display.progress = TRUE,
                   penalty.factor = penalty.factor, method="projection")
  theta_SA <- WPSA(X=X, Y=cond_mu_full, 
                        theta=posterior$theta, force = force,
                 p = 2, model.size = c(1:p),
                 iter=10, temps = 10,
                 pseudo.obs = 0,
                 proposal = proposal.fun,
                 options = list(method = c("selection.variable"),
                                energy.distribution = "boltzman",
                                cooling.schedule = "exponential"),
                 display.progress = TRUE
                  )
  theta_SW <- WPSW(X=X, Y=cond_mu_full, 
                          theta=posterior$theta, force = force, p = 2,
                          direction = "backward", 
                          method="selection.variable",
                   display.progress = TRUE
                  )
  save(theta_proj, theta_selection, theta_SA, theta_SW, file=adapt_file)
} else {
  load(adapt_file)
}

```
```{r w2 distances, message=FALSE, warning=FALSE, echo=FALSE}
models <- list(Selection = theta_selection, 
               Projection = theta_proj,
               "Simulated Annealing" = theta_SA,
               "Backward Stepwise" = theta_SW)
gauss_plot_file <- "w2plot.rds"
if ( !file.exists(gauss_plot_file) ) { #only run if file doesn't exist
  w2plot <- plot.compare(models, cond_mu_full, X, posterior$theta, 
                       "w2", c("posterior","mean"), parallel = TRUE) # this can take a LONG time
  w2plots <- w2plot$plot
  w2plot.dat <- w2plot$data
  saveRDS( w2plot.dat, gauss_plot_file )
} else {
  w2plot.dat <- readRDS( gauss_plot_file )
  w2plots <- lapply(w2plot.dat, function(dd)
                    ggplot( dd, aes(x=nactive, y=dist, color = groups, group=groups )) +
        geom_line() + scale_color_jama() + labs(color ="Method") +
        xlab("Number of active coefficients") + ylab("2-Wasserstein Distance") + theme_bw() +
        scale_x_continuous(expand = c(0, 0)) +
        scale_y_continuous(expand = c(0, 0), limits = c(0, max(dd$dist)*1.1)))
}
```
```{r mse distances, message=FALSE, warning=FALSE, echo=FALSE}
models <- list(Selection = theta_selection, 
               Projection = theta_proj,
               "Simulated Annealing" = theta_SA,
               "Backward Stepwise" = theta_SW)
msefile <- "mseplot.rds"
if(!file.exists(msefile)){
  mseplot_mean <- plot.compare(models, true_mu_full, X, posterior$theta, 
                        "mse", c("mean"), parallel = TRUE)
  mseplot_posterior <- plot.compare(models, c(param$theta, rep(0,p-p_star)), X, posterior$theta, 
                          "mse", c("posterior"), parallel = TRUE)
  mse_dat <- list(posterior = mseplot_posterior$data$posterior,
                  mean = mseplot_mean$data$mean)
  mse_plots <- list(posterior = mseplot_posterior$plot$posterior,
                    mean = mseplot_mean$plot$mean)
  saveRDS(mse_dat, msefile)
} else {
  mse_dat <- readRDS(msefile)
  
  mse_plots <- lapply(mse_dat, function(dd) 
    ggplot( dd, aes(x=nactive, y=dist, color = groups, group=groups )) +
        geom_line() + scale_color_jama() + labs(color ="Method") +
        xlab("Number of active coefficients") + ylab("MSE") + theme_bw() +
        scale_x_continuous(expand = c(0, 0)) +
        scale_y_continuous(expand = c(0, 0), limits = c(0, max(dd$dist)*1.1))
    )
}
```
```{r ridgeplot, echo = FALSE, warning = FALSE, messages = FALSE}
idx <- 512
ridge512 <- ridgePlot(models, index=idx, maxCoef = 10, scale =1 , alpha =0.5, full = cond_mu_full)

ranks <- ranking(models, cond_mu_full, p = 2, maxCoef = 10, quantiles = c(0,0.3, 0.67, 1))
ridgePlots <- ridgePlot(models, index=ranks$index, maxCoef = ranks$ncoef, scale =1 , alpha =0.5, full = cond_mu_full)
```

## Setup
### Predictors
We can measure variable importance in a variety of settings. Our first example is in a setting with Normally distributed data. We generate $n = `r n`$ predictor variables, $X$, from a Multivariate Normal,
\[ X_i \sim N(0, \Sigma), \] with $\Sigma_{i,i} = 1$ for all $i$ from 1 to $p = `r p-1`.$ The first 5 dimensions of $X$ are correlated, dimensions 6-10 are correlated, dimensions 10-20 are correlated, and dimensions 21 to 30 are correlated with correlations `r target$X$corr`. Thus, $\Sigma$ is block diagonal.

### Parameters
We generate parameters as follows
\[ \beta_{0:5} \sim \text{Unif}(1,2),\]
\[ \beta_{6:10} \sim \text{Unif}(-2,-1), \] 
\[ \beta_{11:20} \sim \text{Unif}(0,0.5), \] 
\[ \beta_{21:30} = 0 ,\] and
\[\sigma^2 = 1.\]

### Outcome
We sample the outcome as
\[Y_i \sim N(\beta_0 + X_i^{\top} \beta_{1:20}, \sigma^2) \]

```{r data_echo, echo=TRUE, eval=FALSE}
<<data>>
```

## Posterior Estimation
We put conjugate priors on our parameters,
\[
  \beta_{0:p} \sim N(0,1)
\] and
\[
  \sigma^2 \sim \text{Inv-Gamma}(10,10),
\]
which facillitates quick estimation via known posterior distributions.
```{r post print, eval=FALSE}
hyperparameters <- list(mu = NULL, sigma = NULL,
                          alpha = NULL, beta = NULL,
                        Lambda = NULL)
hyperparameters$mu <- rep(0,p)
hyperparameters$sigma <- diag(1,p,p)
hyperparameters$alpha <- 10
hyperparameters$beta <- 10
hyperparameters$Lambda <- solve(hyperparameters$sigma)

posterior <- target$rpost(nsamp, X, Y, hyperparameters = hyperparameters, method="conjugate")


cond_mu <- x %*% posterior$theta
cond_mu_full <- X %*% posterior$theta
```


We then utilize our 1) selection variable, 2) location/scale, and 3) projection methods to find coarse posteriors close to our full one. Note: this code may take a bit to run.

```{r adapt print, eval = FALSE}
penalty.factor <- rep(1, p)
force <- NULL

theta_selection <- W2L1(X=X, Y = cond_mu_full, 
                        theta=posterior$theta, penalty="selection.lasso",
                        nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                        infimum.maxit=1e4, maxit = 1e3, gamma = gamma,
                        pseudo_observations = 0, display.progress = TRUE,
                 penalty.factor = penalty.factor, method="selection.variable")

theta_proj <- W2L1(X=X, Y=cond_mu_full, 
                        theta=posterior$theta, penalty="mcp",
                        nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                        infimum.maxit=1, maxit = 1e3, gamma = gamma,
                        pseudo_observations = pseudo.observations, display.progress = TRUE,
                 penalty.factor = penalty.factor, method="projection")
theta_SA <- WPSA(X=X, Y=cond_mu_full, 
                      theta=posterior$theta, force = force,
               p = 2, model.size = c(1:p),
               iter=10, temps = 10,
               pseudo.obs = 0,
               proposal = proposal.fun,
               options = list(method = c("selection.variable"),
                              energy.distribution = "boltzman",
                              cooling.schedule = "exponential"),
               display.progress = TRUE
                )
theta_SW <- WPSW(X=X, Y=cond_mu_full, 
                        theta=posterior$theta, force = force, p = 2,
                        direction = "backward", 
                        method="selection.variable",
                 display.progress = TRUE
                )
```


Then we measure our distance from the posterior predictive mean and posterior distribution.


We also measure the mean-squared error from the true mean, since it is known in this case.


Using the Wasserstein distance, we can see how the coarse versions move closer to both the full posterior. The coarse posteriors also converge in MSE to the true parameters 
```{r dist_plot_post,echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=4, out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold', fig.cap="2-Wasserstein distance between the full posterior for the regression coefficients and our coarsened version."}
print(w2plots$posterior)
```

```{r mse_plot_post,echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=4, out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold', fig.cap="MSE between the full posterior for the regression coefficients and the true data generating parameters."}
print(mse_plots$posterior)
```


And we can look at the differences the posterior predictive mean and the full posterior predictive mean, as well as the MSE between the coarse posterior and true mean.
```{r dist_plot_mean,echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=4, out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold', fig.cap="2-Wasserstein distance between coarse and full posterior predictive means and MSE between posterior predictive means and the true means (calculated from the true parameters)"}
print(w2plots$mean)
print(mse_plots$mean)
```


Finally, we can see as more covariates are active, the coarse distribution gets close to the truth. Importantly, it looks like the selection method does better here for this individual $i = 512$. For this person, all of the predictions appear to be exactly the same, so it doesn't matter which method we go with.
```{r ridgeplot_echo, eval=FALSE}
idx <- 512
ridge512 <- ridgePlot(models, index=idx, maxCoef = 10, scale =1 , alpha =0.5, full = cond_mu_full)
```

<!-- However, overall, the projection and location/scale methods do better, which we can see in the above plots. -->
which creates this figure
```{r ridge_plots_single, echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=8, out.width="90%", optipng = '-o7', fig.align = "center"}
print(ridge512)
```

Of course, we may be interested in looking at how good or how bad the predictions from the coarsened posterior can be. Thus, we can calculate how far the coarsened predictions are from the full posterior for each individual in terms of 2-Wasserstein distance, rank the distances, and display the results for say the best and worst performing coarsened predictions, or for any quantile inbetween.

```{r rigeplot_print, echo=FALSE, message=FALSE, warning=FALSE, fig.height= 8, fig.width=8, out.width="50%", optipng = '-o7', fig.align = "center",fig.show='hold', fig.cap = "Ridgeline plots for 1 to 10 coefficients. the top left plot is the observation whose coarsened posterior prediction is closest to the true posterior predictive distribution in terms of 2-Wasserstein distance, and the bottom right is the observations whose is furtherst from the true posterior predictive distribution. Top row, left to right: $0^{th}$ and $33^{rd}$ percentiles. Bottom row, left to right: $67^{th}$ and $100^{th}$ percentiles."}
print(ridgePlots)
```

